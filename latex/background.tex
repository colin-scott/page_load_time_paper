Both browsers and mobile applications typically load content using the HTTP protocol. When a user directs the browser (or application) to a new URL, the browser's Object Loader fetches the root HTML object, as depicted
in Figure~\ref{fig:network-diagram}. The HTML Parser launches additional
fetch requests for each linked resource within the HTML. In this way, the browser incrementally generates the DOM.
As the page loads, the Rendering component paints the UI.

From the user's perspective, the performance of a website can be defined according to a number of different metrics~\cite{above-the-fold,speed-index}. Here,
we focus on page load time, which is easy to measure
and loosely standardized across browsers~\cite{w3c-onload}.

\textbf{Page Load Time}. Roughly speaking, page load time (PLT) is the elapsed time from the moment a user requests a web page to the moment all resources on the page have been loaded~\cite{page-speed}.
We measure PLT by listening to the browser's Javascript \texttt{onload} event,
which fires in most browsers when all resources have been added to the DOM, and all images,
scripts, links, and sub-frames have finished loading~\cite{w3c-onload}.

%\colin{Need to make clear that certain objects (tasks) are dependent on others. When a task is dependent on another, it must wait until its dependencies (tasks) have completed. E.g., all tasks are dependent on the HTML parsing task for the root HTML)}
%\jamshed{Added this to "Critical Path" section


\textbf{Critical Path}. Web pages are comprised of many objects, such as images, Javascript, CSS, and HTML.
Each of these objects is handled by multiple (possibly concurrent) browser tasks: it must be
fetched, parsed or evaluated, and rendered. We refer to the non-overlapping
delays involved in parsing and rendering an object as its `computational
delay,' and refer to the fetch delay as its `network delay.'

Critical path analysis is a method for analyzing the performance of parallel
processes such as browsers.
Certain load tasks are dependent on others and must wait until their
predecessor tasks have completed. The critical path of a web page is the longest chain of dependent browser tasks such that reducing the length of any task not on the critical
path will not change the page load time~\cite{sarkar1987partitioning}.
In Figure~\ref{fig:plt-diagram}, the network and computational delay for the HTML, CSS, JS, and JPEG
objects determine the PLT. If we were to decrease the delay for loading the
PNG object, the critical path would remain the same, and therefore, the PLT would not change.

%\colin{Should make a clearer definition for `tasks' and `objects'. The web page is composed of multiple objects, each of which must be fetched (1 task), parsed or evaluated (1 task), and rendered (1 task).}

% TODO(cs): say whether it's possible that objects on the critical path can
% overlap or not. I think it is possible, e.g., if HTML is partially loaded,
% the browser can kick of other (dependant) tasks before the HTML has completed
% loading.

\subsection{Performance Model}
\label{subsec:model}

We are now in a position to develop a simple performance model, which we can
use to gain an understanding of caching's effect on PLT.
First, consider the following terms:
%We show the details of how we derive
%this model in the Appendix~\ref{sec:appendix}.

\noindent-- Let $X$ denote a given cache hit ratio. We define cache hit ratio as the fraction
of all objects in a web page that are served by a cache. Note that the maximum
value of $X$ is the fraction of cacheable items on the page, which may be less than 1.

\noindent-- Let $K$ denote the fraction of objects on the critical path that
are cacheable.

\noindent-- Let $N$ denote the summation of network fetch delays for all objects on the
critical path for a cold ($X=0$) page load.
% Why do we need to assume the hit ratio is 0 when we just said K is independent of the hit ratio above?
% Colin: N and C are constants, i.e. their value needs to be set (measured)
% with a fixed value of X. It's convenient and simplest to set the fixed value
% to X to 0, i.e. ensure that the initial page load did not have any cached
% hits. We could set the initial value of X to something else, but 0 is simplest.

\noindent-- Let $C$ denote the summation of computational delays for all
objects on the critical path for a cold ($X=0$) page load.

\noindent-- Let $f(X)$ denote the overlap between computational delay and
network delay on the critical path. Normally, dependent objects on the
critical path should not overlap with each other, except for some cases where the
browser can begin concurrently loading an object
when its predecessor is only partially loaded.
For most purposes, we can treat $f(X)$
as being close to 0.

For simplicity, let us assume that (i) the critical path does not change as we vary the cache hit
ratio, (ii) the probability of an object being in cache is uniform
across all cacheable objects, and (iii) cached items incur zero network delay. The probability of an object
on the critical path incurring a network delay is then:
\begin{align*}
1 - Pr(\text{object is cacheable}) \cdot Pr(\text{cache hit})
\end{align*}
%\vspace{0.5em}

The expected value of the PLT for a given $X$ is therefore:
\begin{align*}
E_{PLT}[X] = C + (1 - K \cdot X) \cdot N - f(X)
\end{align*}

\subsection{Model Fitting}
\label{subsec:model_fitting}

\noindent{\bf Fitting $N$ and $C$}

\noindent{\bf Fitting $K$}

% TODO(cs): show off some predictions we can make with this model.
% TODO(cs): put Figure 13 from WProf here.
